# Projeto de IA com Ollama para RecomendaÃ§Ã£o de Vinhos ğŸ·ğŸ§ 

Este projeto utiliza inteligÃªncia artificial generativa com o modelo **LLaMA 3.2**, atravÃ©s da biblioteca [LangChain](https://www.langchain.com/), para responder perguntas relacionadas a vinhos com base em uma base de dados de rÃ³tulos altamente avaliados. A aplicaÃ§Ã£o pode ser executada tanto no terminal quanto por uma interface web utilizando [Streamlit](https://streamlit.io/).

---

## ğŸ§  Tecnologias utilizadas
- Python ğŸ
- [LangChain](https://www.langchain.com/)
- [Ollama](https://ollama.com/) (LLaMA 3.2)
- Streamlit
- ChromaDB com `langchain_chroma` para armazenamento vetorial

---

## âœ¨ Como rodar o projeto

### PrÃ©-requisitos:

1. **Instalar o Ollama localmente**
   - Acesse: https://ollama.com/download e baixe o instalador para o seu sistema operacional.

2. **Instalar o Python e bibliotecas necessÃ¡rias**
   ```bash
   pip install -r requirements.txt
   ```

3. **Instalar o C++ Build Tools (necessÃ¡rio para `langchain_chroma`)**
   - VÃ¡ atÃ©: https://visualstudio.microsoft.com/visual-cpp-build-tools/
   - Durante a instalaÃ§Ã£o, **selecione a opÃ§Ã£o `Desenvolvimento para Desktop com C++`**
   - Isso Ã© necessÃ¡rio para que o `chromadb` funcione corretamente.

4. **Baixar os modelos necessÃ¡rios no Ollama**
   ```bash
   ollama pull llama3.2
   ollama pull mxbai-embed-large
   ```

---

## ğŸš€ Modos de execuÃ§Ã£o

### 1. Rodar pelo terminal:
```bash
python main.py
```
- Interaja diretamente com o agente no terminal.

### 2. Rodar com Streamlit:
```bash
streamlit run app.py
```
- Interface web amigÃ¡vel para perguntas e respostas sobre vinhos ğŸ·

---

## ğŸ“‚ Estrutura dos arquivos principais
- `main.py` â†’ versÃ£o para terminal
- `app.py` â†’ versÃ£o web com Streamlit
- `vector.py` â†’ carrega a base de dados de vinhos e cria o banco vetorial
- `top_rated_wines.csv` â†’ base de dados com os melhores vinhos avaliados

---
