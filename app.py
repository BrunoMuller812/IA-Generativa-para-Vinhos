import streamlit as st
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever

# Modelo LLM
model = OllamaLLM(model="llama3.2")

# Prompt
template = """
VocÃª Ã© um especialista em responder perguntas sobre vinhos.

Aqui estÃ¡ a sua base de dados para consulta: {wine_data}
VocÃª deve responder a pergunta a seguir com base nesses dados,
caso vocÃª nÃ£o encontre um vinho nessa base responda que nÃ£o encontrou
e nÃ£o procure na internet.

Somente responda perguntas relacionadas a vinho, caso uma pergunta de outro
tÃ³pico seja feita, nÃ£o a responda.

Aqui estÃ¡ a pergunta que vocÃª deve responder: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

# Interface Streamlit
st.set_page_config(page_title="Especialista em Vinhos", page_icon="ğŸ·")

st.title("Especialista em Vinhos ğŸ·")
st.write("Digite uma pergunta relacionada a vinhos e receba uma resposta baseada na base de dados dos vinhos mais bem avaliados.")

# Input do usuÃ¡rio
question = st.text_input("FaÃ§a sua pergunta:", placeholder="Ex: Me indique um vinho chileno")

if question:
    with st.spinner("Consultando especialista..."):
        wine_data = retriever.invoke(question)
        result = chain.invoke({"wine_data": wine_data, "question": question})
        st.success("Resposta:")
        st.write(result)
